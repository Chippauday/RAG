# -*- coding: utf-8 -*-
"""RAG_Based_Search_Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGIQPbHdLHDMxFOwheDZVYaCgkW97LFS

Install Dependencies
"""

!pip install langchain langchain-openai langchain-core
!pip install langchain-community gradio

"""Data Loading, Chunking, and Embedding"""

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Example: load a web page (replace with your data source)
loader = WebBaseLoader("https://en.wikipedia.org/wiki/Artificial_intelligence")
docs = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

"""LLM API Key Connenction And VectorDB"""

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from google.colab import userdata

# Initialize embedding model using Gemini
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=userdata.get('GOOGLE_API_KEY'))

# Create vector store and add chunks
vector_store = InMemoryVectorStore(embeddings)
vector_store.add_documents(chunks)

"""Input Query"""

query = "how autnomous ai agent works?"
retriever = vector_store.as_retriever()
relevant_docs = retriever.get_relevant_documents(query)

"""To use the Gemini API, add your API key to the secrets manager under the "ðŸ”‘" in the left panel. Give it the name `GOOGLE_API_KEY`. Then pass the key to the SDK:

Output Generation
"""

from langchain_google_genai import ChatGoogleGenerativeAI
from google.colab import userdata

# Initialize chat model using Google Gemini
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=userdata.get('GOOGLE_API_KEY'))

# Compose prompt with retrieved context
context = "\n".join(doc.page_content for doc in relevant_docs)
prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"

# Generate answer
response = llm.invoke(prompt)
print(response.content)

"""You can display the sources of the retrieved chunks to increase transparency"""

for doc in relevant_docs:
    print(f"Source: {doc.metadata.get('source', 'unknown')}")

def rag_qa(query):
    """
    Performs RAG to answer a query and includes sources.
    """
    # Retrieve relevant documents
    retriever = vector_store.as_retriever()
    relevant_docs = retriever.get_relevant_documents(query)

    # Compose prompt with retrieved context
    context = "\n".join(doc.page_content for doc in relevant_docs)
    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"

    # Generate answer
    response = llm.invoke(prompt)
    answer = response.content

    # Add sources to the answer
    sources = "\nSources:\n" + "\n".join(f"- {doc.metadata.get('source', 'unknown')}" for doc in relevant_docs)

    return answer + sources

import gradio as gr
iface = gr.Interface(
    fn=rag_qa,
    inputs=gr.Textbox(label="Enter your question"),
    outputs=gr.Textbox(label="Answer with sources"),
    title="GEN-AI RAG Based Search Engine",
    description="Ask a question. The engine retrieves relevant info and generates a grounded answer with sources."
)

iface.launch()